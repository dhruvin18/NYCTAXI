# -*- coding: utf-8 -*-
"""Producer_NYCTaxiHotZonePrediction (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bA4IvkkNjY91McigTnjD8A41u2bK0NNy
"""

KAFKA_BOOTSTRAP_SERVERS = "pkc-619z3.us-east1.gcp.confluent.cloud:9092"
KAFKA_API_KEY          = "RUCV7DWIQGCQOMBG"
KAFKA_API_SECRET       = "7f/Yq4epS6X8nR2uV0qpf4Je0rBS3BfkgYcgHiDp5t/HElmEs2dOEWmLN1QMIBjU"
TOPIC                  = "nyc_taxi"

from google.colab import drive
import os

import pandas as pd

from pyspark import *
from pyspark.sql import *
from pyspark.sql.functions import col, floor, expr, avg, count, sum, hour, date_format, to_timestamp, window, dayofweek, lag, desc, sequence, explode, lit, date_trunc, from_unixtime, unix_timestamp, when, lead

from pyspark.ml.feature import StringIndexer, OneHotEncoder
from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import RandomForestRegressor, LinearRegression, DecisionTreeRegressor
from pyspark.ml.evaluation import RegressionEvaluator

spark = SparkSession.builder.appName('nyctaxi_nilay').getOrCreate()

zone_lookup_df = spark.read.csv(
    "taxi_zone_lookup.csv",  # update path if needed
    header=True,
    inferSchema=True
)

# Preview
zone_lookup_df.show(5, truncate=False)
zone_lookup_df.printSchema()

# Single Parquet file path
file_path = 'yellow_tripdata_2025-02.parquet'

# Read the file
print(f"ðŸ“„ Reading file: {file_path}")
df = spark.read.parquet(file_path)

# Fix inconsistent column name
if 'Airport_fee' in df.columns:
    df = df.withColumnRenamed('Airport_fee', 'airport_fee')

# Standardize schema types
base_df = df \
    .withColumn("VendorID", col("VendorID").cast("integer")) \
    .withColumn("tpep_pickup_datetime", col("tpep_pickup_datetime").cast("timestamp")) \
    .withColumn("tpep_dropoff_datetime", col("tpep_dropoff_datetime").cast("timestamp")) \
    .withColumn("passenger_count", col("passenger_count").cast("integer")) \
    .withColumn("trip_distance", col("trip_distance").cast("double")) \
    .withColumn("RatecodeID", col("RatecodeID").cast("integer")) \
    .withColumn("store_and_fwd_flag", col("store_and_fwd_flag").cast("string")) \
    .withColumn("PULocationID", col("PULocationID").cast("integer")) \
    .withColumn("DOLocationID", col("DOLocationID").cast("integer")) \
    .withColumn("payment_type", col("payment_type").cast("integer")) \
    .withColumn("fare_amount", col("fare_amount").cast("double")) \
    .withColumn("extra", col("extra").cast("double")) \
    .withColumn("mta_tax", col("mta_tax").cast("double")) \
    .withColumn("tip_amount", col("tip_amount").cast("double")) \
    .withColumn("tolls_amount", col("tolls_amount").cast("double")) \
    .withColumn("improvement_surcharge", col("improvement_surcharge").cast("double")) \
    .withColumn("total_amount", col("total_amount").cast("double")) \
    .withColumn("congestion_surcharge", col("congestion_surcharge").cast("double")) \
    .withColumn("airport_fee", col("airport_fee").cast("double"))

# Show schema and sample records
base_df.printSchema()
base_df.show(5)

base_df_cleaned = base_df.filter(base_df["total_amount"] > 5)

# Optional: Check how many rows were dropped
print("Original count:", base_df.count())
print("Filtered count:", base_df_cleaned.count())

from pyspark.sql.types import TimestampType

# Creating grid of zone and time bins
start_time = "2025-02-01 00:00:00"
end_time = "2025-02-28 23:30:00"

# Create time range DataFrame
time_bins_df = spark.createDataFrame([(start_time, end_time)], ["start", "end"]) \
    .withColumn("start", col("start").cast(TimestampType())) \
    .withColumn("end", col("end").cast(TimestampType())) \
    .select(explode(sequence(
        date_trunc("minute", col("start")),
        date_trunc("minute", col("end")),
        expr("interval 30 minutes")
    )).alias("time_bin"))

print(time_bins_df.count())
zone_ids_df = zone_lookup_df.select(col("LocationID").alias("src")).distinct()
print(zone_ids_df.count())
zone_time_grid_df = zone_ids_df.crossJoin(time_bins_df)
#zone_time_grid_df.orderBy("time_bin","src").show(265, truncate=False)
print(zone_time_grid_df.count())

base_df_cleaned = base_df_cleaned.withColumn("pickup_ts", to_timestamp("tpep_pickup_datetime"))

trip_df_binned = base_df_cleaned.withColumn(
    "time_bin",
    from_unixtime(
        (unix_timestamp("tpep_pickup_datetime") / 1800).cast("int") * 1800
    ).cast("timestamp")
)

# Step 2: Aggregate trip data by (src, time_bin)
agg_df_temp = trip_df_binned.groupBy(
    col("PULocationID").alias("src"),
    "time_bin"
).agg(
    count("*").alias("trip_count"),
    avg("trip_distance").alias("avg_trip_distance"),
    avg("total_amount").alias("avg_total_amount")
)

# Step 3: Left join with zone-time grid to ensure full coverage
agg_df = zone_time_grid_df.join(agg_df_temp, on=["src", "time_bin"], how="left")

# Step 4: Fill nulls where no trips occurred
agg_df = agg_df.fillna({
    "trip_count": 0,
    "avg_trip_distance": 0.0,
    "avg_total_amount": 0.0
})

# Step 5: Add time-based features (based on time_bin)
agg_df = agg_df \
    .withColumn("hour_of_day", hour("time_bin")) \
    .withColumn("day_of_week", dayofweek("time_bin")) \
    .withColumn("is_weekend", expr("day_of_week IN (1, 7)")) \
    .withColumn("is_night", expr("hour_of_day < 6 OR hour_of_day >= 22"))

# Optional: Preview
agg_df.orderBy("time_bin", "src").show(10, truncate=False)

agg_df.filter("trip_count>1").orderBy("time_bin").show(5, truncate=False)

# Define window spec
window_spec = Window.partitionBy("src").orderBy("time_bin")

# Add lag features + keep relevant columns
agg_df_lagged = agg_df \
    .withColumn("trip_count_lag_1", lag("trip_count", 1).over(window_spec)) \
    .withColumn("trip_count_lag_2", lag("trip_count", 2).over(window_spec)) \
    .withColumn("avg_total_amount_lag_1", lag("avg_total_amount", 1).over(window_spec)) \
    .withColumn("avg_total_amount_lag_2", lag("avg_total_amount", 2).over(window_spec)) \
    .dropna(subset=[
        "trip_count_lag_1", "trip_count_lag_2",
        "avg_total_amount_lag_1", "avg_total_amount_lag_2"
    ]) \
    .select(
        "src", "time_bin", "trip_count","avg_total_amount",
        "trip_count_lag_1", "trip_count_lag_2",
        "avg_total_amount_lag_1", "avg_total_amount_lag_2",
        "hour_of_day", "day_of_week", "is_weekend", "is_night"
    )

agg_df_lagged.orderBy(desc("trip_count")).show(10, truncate=False)

from pyspark.sql.functions import when

# Define hour bucket categories
agg_df_lagged = agg_df_lagged.withColumn(
    "hour_bucket",
    when((col("hour_of_day") >= 0) & (col("hour_of_day") < 6), lit(0))  # Early Morning
    .when((col("hour_of_day") >= 6) & (col("hour_of_day") < 10), lit(1))  # Morning Rush
    .when((col("hour_of_day") >= 10) & (col("hour_of_day") < 16), lit(2))  # Midday
    .when((col("hour_of_day") >= 16) & (col("hour_of_day") < 20), lit(3))  # Evening Rush
    .otherwise(lit(4))  # Late Evening (20-23)
)

# Preview the result
agg_df_lagged.select("src", "time_bin", "hour_of_day", "hour_bucket").show(10, truncate=False)

# Convert boolean columns to numeric (1 for True, 0 for False)
agg_df_lagged = agg_df_lagged \
    .withColumn("is_weekend_numeric", when(col("is_weekend") == True, 1).otherwise(0)) \
    .withColumn("is_night_numeric", when(col("is_night") == True, 1).otherwise(0))

# Preview
agg_df_lagged.select("is_weekend", "is_weekend_numeric", "is_night", "is_night_numeric").show(5, truncate=False)

# Define window over time for each source location
window_spec = Window.partitionBy("src").orderBy("time_bin")
# Add label column (next hour's trip_count)
agg_df_lagged = agg_df_lagged.withColumn("target_label", lead("trip_count", 1).over(window_spec))
# Preview
agg_df_lagged.orderBy("src", "time_bin").show(10, truncate=False)

!pip install --quiet confluent-kafka

# Producer for Kafka

from confluent_kafka import Producer
import json, time

p = Producer({
    'bootstrap.servers': KAFKA_BOOTSTRAP_SERVERS,
    'security.protocol': 'SASL_SSL',
    'sasl.mechanisms': 'PLAIN',
    'sasl.username': KAFKA_API_KEY,
    'sasl.password': KAFKA_API_SECRET,
})


# Load and sort data
# Convert 'time_bin' column to datetime in agg_df_lagged
agg_df_lagged = agg_df_lagged.withColumn("time_bin", to_timestamp("time_bin"))
# Convert to Pandas DataFrame
df_prod = agg_df_lagged.toPandas()
df_prod["time_bin"] = pd.to_datetime(df_prod["time_bin"])
df_prod = df_prod.sort_values(by="time_bin")


# Group by 30-min time_bin chunks
grouped = df_prod.groupby("time_bin")

# Stream grouped records to Kafka
for time_bin, group_df in grouped:
    for _, row in group_df.iterrows():
        message = {
            "src": row.src,
            "time_bin": row.time_bin.strftime("%Y-%m-%d %H:%M:%S"),
            "trip_count": row.trip_count,
            "avg_total_amount": row.avg_total_amount,
            "trip_count_lag_1": row.trip_count_lag_1,
            "trip_count_lag_2": row.trip_count_lag_2,
            "avg_total_amount_lag_1": row.avg_total_amount_lag_1,
            "avg_total_amount_lag_2": row.avg_total_amount_lag_2,
            "hour_of_day": row.hour_of_day,
            "day_of_week": row.day_of_week,
            "is_weekend": row.is_weekend,
            "is_night": row.is_night,
        }
        try:
            p.produce(TOPIC, value=json.dumps(message))
            p.poll(0)
        except BufferError:
            print("Buffer full, waiting...")
            time.sleep(1)

    print(f"Streamed 30-min batch for: {time_bin}")
    time.sleep(1)

p.flush()
print("All data streamed to topic 'nyc-hotzones'")



