# -*- coding: utf-8 -*-
"""NYCTaxiHotZonePrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17ORUgxqat6kAsjQZ9xcaya4ZiQz0PMW8
"""

#Installing the necessary libraries
!apt-get install openjdk-11-jdk-headless -qq > /dev/null
!wget -q https://bitbucket.org/habedi/datasets/raw/b6769c4664e7ff68b001e2f43bc517888cbe3642/spark/spark-3.0.2-bin-hadoop2.7.tgz
!tar xf spark-3.0.2-bin-hadoop2.7.tgz
!rm -rf spark-3.0.2-bin-hadoop2.7.tgz*
!pip -q install findspark pyspark graphframes

from google.colab import drive
import os

os.environ["PYSPARK_DRIVER_PYTHON"] = "jupyter"
os.environ["PYSPARK_DRIVER_PYTHON_OPTS"] = "notebook"
os.environ["PYSPARK_SUBMIT_ARGS"] = "--packages graphframes:graphframes:0.8.1-spark3.0-s_2.12 pyspark-shell"

drive.mount('/content/drive')

import pandas as pd
from graphframes import *
from pyspark import *
from pyspark.sql import *
from pyspark.sql.functions import col, floor, expr, avg, count, sum, hour, date_format, to_timestamp, window, dayofweek, lag, desc, sequence, explode, lit, date_trunc, from_unixtime, unix_timestamp, when, lead, to_date
from pyspark.sql.types import TimestampType
import datetime

from pyspark.ml.feature import StringIndexer, OneHotEncoder
from pyspark.ml import Pipeline
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import RandomForestRegressor, LinearRegression, DecisionTreeRegressor
from pyspark.ml.evaluation import RegressionEvaluator

import plotly.express as px
import plotly.graph_objects as go
import json

spark = SparkSession.builder.appName('nyctaxi').getOrCreate()

zone_lookup_df = spark.read.csv(
    "/content/drive/MyDrive/TLC_NYC/taxi_zone_lookup.csv",  # update path if needed
    header=True,
    inferSchema=True
)

# Preview
zone_lookup_df.show(5, truncate=False)
zone_lookup_df.printSchema()

year_list = ["/content/drive/MyDrive/TLC_NYC/2024","/content/drive/MyDrive/TLC_NYC/2025"]

# Initialize base DataFrame
base_df = None

# Loop through each Parquet file in the current year folder
for year_path in year_list:
  for file in os.listdir(year_path):
      if file.endswith(".parquet"):
          file_path = os.path.join(year_path, file)
          print(f"  ðŸ“„ Reading file: {file}")

          # Read and normalize schema
          df_temp = spark.read.parquet(file_path)

          # Fix inconsistent column name
          if 'Airport_fee' in df_temp.columns:
              df_temp = df_temp.withColumnRenamed('Airport_fee', 'airport_fee')

          # Cast all columns to standard schema
          df_temp = df_temp \
              .withColumn("VendorID", col("VendorID").cast("integer")) \
              .withColumn("tpep_pickup_datetime", col("tpep_pickup_datetime").cast("timestamp")) \
              .withColumn("tpep_dropoff_datetime", col("tpep_dropoff_datetime").cast("timestamp")) \
              .withColumn("passenger_count", col("passenger_count").cast("integer")) \
              .withColumn("trip_distance", col("trip_distance").cast("double")) \
              .withColumn("RatecodeID", col("RatecodeID").cast("integer")) \
              .withColumn("store_and_fwd_flag", col("store_and_fwd_flag").cast("string")) \
              .withColumn("PULocationID", col("PULocationID").cast("integer")) \
              .withColumn("DOLocationID", col("DOLocationID").cast("integer")) \
              .withColumn("payment_type", col("payment_type").cast("integer")) \
              .withColumn("fare_amount", col("fare_amount").cast("double")) \
              .withColumn("extra", col("extra").cast("double")) \
              .withColumn("mta_tax", col("mta_tax").cast("double")) \
              .withColumn("tip_amount", col("tip_amount").cast("double")) \
              .withColumn("tolls_amount", col("tolls_amount").cast("double")) \
              .withColumn("improvement_surcharge", col("improvement_surcharge").cast("double")) \
              .withColumn("total_amount", col("total_amount").cast("double")) \
              .withColumn("congestion_surcharge", col("congestion_surcharge").cast("double")) \
              .withColumn("airport_fee", col("airport_fee").cast("double"))

          # Union with base DataFrame
          if base_df is None:
              base_df = df_temp
          else:
              base_df = base_df.unionByName(df_temp, allowMissingColumns=True)

base_df.printSchema()
base_df.show(5)

base_df_cleaned = base_df.filter(base_df["total_amount"] > 5)

# Optional: Check how many rows were dropped
print("Original count:", base_df.count())
print("Filtered count:", base_df_cleaned.count())

start_time = "2024-11-01 00:00:00"
end_time = "2025-01-31 23:30:00"

# Create time range DataFrame
time_bins_df = spark.createDataFrame([(start_time, end_time)], ["start", "end"]) \
    .withColumn("start", col("start").cast(TimestampType())) \
    .withColumn("end", col("end").cast(TimestampType())) \
    .select(explode(sequence(
        date_trunc("minute", col("start")),
        date_trunc("minute", col("end")),
        expr("interval 30 minutes")
    )).alias("time_bin"))

print(time_bins_df.count())
zone_ids_df = zone_lookup_df.select(col("LocationID").alias("src")).distinct()
print(zone_ids_df.count())
zone_time_grid_df = zone_ids_df.crossJoin(time_bins_df)
#zone_time_grid_df.orderBy("time_bin","src").show(265, truncate=False)
print(zone_time_grid_df.count())

base_df_cleaned = base_df_cleaned.withColumn("pickup_ts", to_timestamp("tpep_pickup_datetime"))
trip_df_binned = base_df_cleaned.withColumn(
    "time_bin",
    from_unixtime(
        (unix_timestamp("pickup_ts") / 1800).cast("int") * 1800
    ).cast("timestamp")
)

# Step 2: Aggregate trip data by (src, time_bin)
agg_df_temp = trip_df_binned.groupBy(
    col("PULocationID").alias("src"),
    "time_bin"
).agg(
    count("*").alias("trip_count"),
    avg("trip_distance").alias("avg_trip_distance"),
    avg("total_amount").alias("avg_total_amount")
)

# Step 3: Left join with zone-time grid to ensure full coverage
agg_df = zone_time_grid_df.join(agg_df_temp, on=["src", "time_bin"], how="left")

# Step 4: Fill nulls where no trips occurred
agg_df = agg_df.fillna({
    "trip_count": 0,
    "avg_trip_distance": 0.0,
    "avg_total_amount": 0.0
})

# Step 5: Add time-based features (based on time_bin)
agg_df = agg_df \
    .withColumn("hour_of_day", hour("time_bin")) \
    .withColumn("day_of_week", dayofweek("time_bin")) \
    .withColumn("is_weekend", expr("day_of_week IN (1, 7)")) \
    .withColumn("is_night", expr("hour_of_day < 6 OR hour_of_day >= 22"))

# Optional: Preview
agg_df.orderBy("time_bin", "src").show(5, truncate=False)

agg_df.filter("trip_count>1").orderBy("time_bin").show(5, truncate=False)

# Define window spec
window_spec = Window.partitionBy("src").orderBy("time_bin")

# Add lag features + keep relevant columns
agg_df_lagged = agg_df \
    .withColumn("trip_count_lag_1", lag("trip_count", 1).over(window_spec)) \
    .withColumn("trip_count_lag_2", lag("trip_count", 2).over(window_spec)) \
    .withColumn("avg_total_amount_lag_1", lag("avg_total_amount", 1).over(window_spec)) \
    .withColumn("avg_total_amount_lag_2", lag("avg_total_amount", 2).over(window_spec)) \
    .dropna(subset=[
        "trip_count_lag_1", "trip_count_lag_2",
        "avg_total_amount_lag_1", "avg_total_amount_lag_2"
    ]) \
    .select(
        "src", "time_bin", "trip_count","avg_total_amount",
        "trip_count_lag_1", "trip_count_lag_2",
        "avg_total_amount_lag_1", "avg_total_amount_lag_2",
        "hour_of_day", "day_of_week", "is_weekend", "is_night"
    )

agg_df_lagged.orderBy(desc("trip_count")).show(10, truncate=False)

agg_df_lagged.orderBy(("avg_total_amount")).show(10, truncate=False)

agg_df_lagged.filter("src=49").orderBy(desc("time_bin")).show(10, truncate=False)

from pyspark.sql.functions import when

# Define hour bucket categories
agg_df_lagged = agg_df_lagged.withColumn(
    "hour_bucket",
    when((col("hour_of_day") >= 0) & (col("hour_of_day") < 6), lit(0))  # Early Morning
    .when((col("hour_of_day") >= 6) & (col("hour_of_day") < 10), lit(1))  # Morning Rush
    .when((col("hour_of_day") >= 10) & (col("hour_of_day") < 16), lit(2))  # Midday
    .when((col("hour_of_day") >= 16) & (col("hour_of_day") < 20), lit(3))  # Evening Rush
    .otherwise(lit(4))  # Late Evening (20-23)
)

# Preview the result
agg_df_lagged.select("src", "time_bin", "hour_of_day", "hour_bucket").show(10, truncate=False)

# Convert boolean columns to numeric (1 for True, 0 for False)
agg_df_lagged = agg_df_lagged \
    .withColumn("is_weekend_numeric", when(col("is_weekend") == True, 1).otherwise(0)) \
    .withColumn("is_night_numeric", when(col("is_night") == True, 1).otherwise(0))

# Preview
agg_df_lagged.select("is_weekend", "is_weekend_numeric", "is_night", "is_night_numeric").show(5, truncate=False)

# Define window over time for each source location
window_spec = Window.partitionBy("src").orderBy("time_bin")
# Add label column (next hour's trip_count)
agg_df_lagged = agg_df_lagged.withColumn("target_label", lead("trip_count", 1).over(window_spec))
# Preview
agg_df_lagged.orderBy("src", "time_bin").show(265, truncate=False)

# Remove rows where label is null (last time_bin per src)
agg_df_lagged = agg_df_lagged.filter(agg_df_lagged["target_label"].isNotNull())
agg_df_lagged.orderBy("src", "time_bin").show(48, truncate=False)

# Index the categorical columns
indexer_src = StringIndexer(inputCol="src", outputCol="src_index")
indexer_day_of_week = StringIndexer(inputCol="day_of_week", outputCol="day_of_week_index")
indexer_hour_bucket = StringIndexer(inputCol="hour_bucket", outputCol="hour_bucket_index")

# OneHotEncoder for the indexed columns
encoder_src = OneHotEncoder(inputCol="src_index", outputCol="src_onehot")
encoder_day_of_week = OneHotEncoder(inputCol="day_of_week_index", outputCol="day_of_week_onehot")
encoder_hour_bucket = OneHotEncoder(inputCol="hour_bucket_index", outputCol="hour_bucket_onehot")

feature_cols = [
    "trip_count", "avg_total_amount",
    "trip_count_lag_1", "trip_count_lag_2",
    "avg_total_amount_lag_1", "avg_total_amount_lag_2",
    "is_weekend_numeric", "is_night_numeric",
    "src_onehot", "day_of_week_onehot", "hour_bucket_onehot"
]

# Create vector assembler
assembler = VectorAssembler(
    inputCols=feature_cols,
    outputCol="features"
)

pipeline = Pipeline(stages=[
    indexer_src, indexer_day_of_week, indexer_hour_bucket,
    encoder_src, encoder_day_of_week, encoder_hour_bucket,
    assembler
])

# Fit and transform
pipeline_model = pipeline.fit(agg_df_lagged)
processed_df = pipeline_model.transform(agg_df_lagged)

cutoff = to_timestamp(lit("2025-01-21 00:00:00"))

# Split DataFrame based on time_bin
train_df = processed_df.filter(col("time_bin") < cutoff)
test_df = processed_df.filter(col("time_bin") >= cutoff)

# Optional: Confirm split
print("Train count:", train_df.count())
print("Test count:", test_df.count())

# Preview
train_df.orderBy("time_bin").show(3, truncate=False)
test_df.orderBy("time_bin").show(3, truncate=False)

def evaluate_model(predictions, prediction_col):
    evaluators = {
        "RMSE": RegressionEvaluator(labelCol="target_label", predictionCol=prediction_col, metricName="rmse"),
        "MAE": RegressionEvaluator(labelCol="target_label", predictionCol=prediction_col, metricName="mae"),
        "R2": RegressionEvaluator(labelCol="target_label", predictionCol=prediction_col, metricName="r2")
    }
    return {metric: evaluator.evaluate(predictions) for metric, evaluator in evaluators.items()}

rf = RandomForestRegressor(featuresCol="features", labelCol="target_label", predictionCol="rf_prediction")
rf_model = rf.fit(train_df)
rf_preds = rf_model.transform(test_df)
rf_metrics = evaluate_model(rf_preds, "rf_prediction")
print("Random Forest:", rf_metrics)

lr = LinearRegression(featuresCol="features", labelCol="target_label", predictionCol="lr_prediction")
lr_model = lr.fit(train_df)
lr_preds = lr_model.transform(test_df)
lr_metrics = evaluate_model(lr_preds, "lr_prediction")
print("Linear Regression:", lr_metrics)

dt = DecisionTreeRegressor(featuresCol="features", labelCol="target_label", predictionCol="dt_prediction")
dt_model = dt.fit(train_df)
dt_preds = dt_model.transform(test_df)
dt_metrics = evaluate_model(dt_preds, "dt_prediction")
print("Decision Tree:", dt_metrics)

model_save_path = "/content/drive/MyDrive/TLC_NYC/NYCModels"

rf_model.write().overwrite().save(f"{model_save_path}/RandomForestModel")
lr_model.write().overwrite().save(f"{model_save_path}/LinearRegressionModel")
dt_model.write().overwrite().save(f"{model_save_path}/DecisionTreeModel")

lr_preds.select("src","time_bin","target_label","lr_prediction").orderBy(desc("target_label")).show(20, truncate=False)

import geopandas as gpd

# Replace the filename if it's different
gdf = gpd.read_file("/content/drive/MyDrive/TLC_NYC/ZonesGeoLocation.json")

# Show the first few rows
gdf.head()

zones = pd.read_csv("/content/drive/MyDrive/TLC_NYC/taxi_zone_lookup.csv")
zones.head()
len(zones)

geozones = zones.join(gdf, on="LocationID", how="inner", lsuffix="_left", rsuffix="_right")
len(geozones)

jan31_pd = (
    lr_preds
    .filter(to_date("time_bin") == "2025-01-31")
    .select("src", "time_bin", "target_label", "lr_prediction")
    .toPandas()
)

# Step 2: Get sorted time bins
time_bins = sorted(jan31_pd["time_bin"].unique())

# Step 3: Build top-10 predicted and actual data for each time bin
frames = []

for tb in time_bins:
    filtered = jan31_pd[jan31_pd["time_bin"] == tb]

    top_pred = filtered.sort_values("lr_prediction", ascending=False).head(10)[["src", "lr_prediction"]].copy()
    top_actual = filtered.sort_values("target_label", ascending=False).head(10)[["src", "target_label"]].copy()

    top_pred["rank"] = range(1, 11)
    top_actual["rank"] = range(1, 11)

    top_pred["type"] = "Predicted"
    top_actual["type"] = "Actual"

    top_pred["time_bin"] = tb
    top_actual["time_bin"] = tb

    top_pred.rename(columns={"lr_prediction": "value"}, inplace=True)
    top_actual.rename(columns={"target_label": "value"}, inplace=True)

    frames.append(top_pred)
    frames.append(top_actual)

# Step 4: Combine all into one DataFrame
df = pd.concat(frames)

time_to_plot = time_bins[0]

subset = df[df["time_bin"] == time_to_plot]

fig = px.bar(
    subset,
    x="src",
    y="value",
    color="type",
    barmode="group",
    title=f"Top 10 Hot Zones at {time_to_plot}",
    labels={"src": "Zone ID", "value": "Trip Count"}
)
fig.show()

fig = px.bar(
    df,
    x="src",
    y="value",
    color="type",
    animation_frame="time_bin",
    barmode="group",
    title="Top 10 Hot Zones per Time Interval on 31-Jan",
    labels={"src": "Zone ID", "value": "Trip Count"}
)
fig.show()

df["src"] = df["src"].astype(int)  # Ensure int type for join
df_poly = df.merge(gdf, left_on="src", right_on="LocationID", how="left")

geojson_gdf = json.loads(gdf.to_json())

fig = px.choropleth_mapbox(
    df_poly,
    geojson=geojson_gdf,
    locations="LocationID",
    featureidkey="properties.LocationID",
    color="type",  # << CATEGORICAL: "Actual" or "Predicted"
    hover_name="zone",
    animation_frame="time_bin",  # Optional: for animation
    mapbox_style="carto-positron",
    center={"lat": 40.758, "lon": -73.9860},
    zoom=11.5,
    opacity=0.6,
    color_discrete_map={
        "Actual": "red",
        "Predicted": "green"
    },
    title="Top 10 Hot Zones â€” Predicted vs Actual"
)
fig.update_layout(height=700)
fig.show()

# Choose one time interval to visualize
time_to_plot = df_poly["time_bin"].unique()[5]  # or specify one manually
subset = df_poly[df_poly["time_bin"] == time_to_plot]

fig = px.choropleth_mapbox(
    subset,
    geojson=geojson_gdf,
    locations="LocationID",
    featureidkey="properties.LocationID",
    color="type",  # 'Predicted' or 'Actual'
    hover_name="zone",
    mapbox_style="carto-positron",
    center={"lat": 40.75, "lon": -73.98},  # More accurate center near Midtown Manhattan
    zoom=11.5,  # Zoomed-in on NYC core
    opacity=0.6,
    color_discrete_map={
        "Actual": "red",
        "Predicted": "green"
    },
    title=f"Top 10 Hot Zones â€” {time_to_plot} (Predicted vs Actual)"
)

# Set custom height to make the map taller
fig.update_layout(height=700)

fig.show()

hit_rates = []

# Loop over each time_bin
for tb in time_bins:
    # Filter for the current time bin
    subset = df[df["time_bin"] == tb]

    # Extract top 10 predicted and actual src values
    pred_srcs = set(subset[(subset["type"] == "Predicted")]["src"])
    actual_srcs = set(subset[(subset["type"] == "Actual")]["src"])

    # Compute intersection
    hits = pred_srcs.intersection(actual_srcs)
    hit_rate = len(hits) / 10

    hit_rates.append({
        "time_bin": tb,
        "hit_rate": hit_rate,
        "hits": hits
    })

# Convert to DataFrame for inspection
hit_rate_df = pd.DataFrame(hit_rates)

from matplotlib import pyplot as plt
import seaborn as sns
def _plot_series(series, series_name, series_index=0):
  palette = list(sns.palettes.mpl_palette('Dark2'))
  xs = series['time_bin']
  ys = series['hit_rate']

  plt.plot(xs, ys, label=series_name, color=palette[series_index % len(palette)])

fig, ax = plt.subplots(figsize=(10, 5.2), layout='constrained')
df_sorted = hit_rate_df.sort_values('time_bin', ascending=True)
_plot_series(df_sorted, '')
sns.despine(fig=fig, ax=ax)
plt.xlabel('time_bin')
_ = plt.ylabel('hit_rate')